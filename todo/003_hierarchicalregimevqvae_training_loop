Strategic Plan: Developing the HierarchicalRegimeVQVAE Training Loop

  1. Understanding the Goal

  The objective is to develop a comprehensive training loop for the HierarchicalRegimeVQVAE model. This involves implementing the core mechanisms required to train the
  model, including:
   * Defining and calculating the necessary loss functions (e.g., reconstruction loss, VQ commitment loss, codebook loss).
   * Setting up and executing the optimization process (e.g., using an Adam optimizer).
   * Establishing a robust data handling pipeline for feeding time series data to the model.
   * Implementing the logic for managing and updating the VQ-VAE's codebooks across its hierarchical layers.

  This foundational training loop will enable the HierarchicalRegimeVQVAE to learn its library of discrete regimes. Once this core loop is functional and verified, the
  integration of Continual Learning (EWC) mechanisms will be the subsequent step.

  2. Investigation & Analysis

  To effectively design the training loop, a thorough understanding of the HierarchicalRegimeVQVAE's structure, its inputs, outputs, and the project's data handling
  conventions is essential.

  Investigation Steps:

   1. Read `sasaie_core/models/hierarchical_regime_vq_vae.py`:
       * Purpose: Understand the HierarchicalRegimeVQVAE class structure, its __init__ method, forward pass, and especially the hierarchical_encode and hierarchical_decode
         methods. Identify the inputs and outputs of each ContinualVQVAELayer.
       * Search: Look for any existing placeholders or comments related to training, loss calculation, or codebook updates.
       * Critical Questions:
           * What are the expected input dimensions and format for the forward pass (which will likely take features from HierarchicalStreamingMP)?
           * How does the forward pass produce reconstruction, quantized latents, and potentially loss components (e.g., commitment loss)?
           * How are the codebooks accessed and updated within the ContinualVQVAELayer?
           * Are there any existing methods for calculating VQ-VAE specific losses (reconstruction, commitment, codebook)?

   2. Read `sasaie_core/components/perception.py`:
       * Purpose: Understand the output format of HierarchicalStreamingMP.update and HierarchicalStreamingMP.get_latest_profile. This will be the input to the VQ-VAE.
       * Critical Questions:
           * What is the exact structure and data type of the mp_features dictionary (scale -> torch.Tensor) that HierarchicalRegimeVQVAE expects?
           * How can we simulate or generate this input data for initial testing of the training loop?

   3. Read `sasaie_core/pipeline.py`:
       * Purpose: Understand how data currently flows into the system and how HierarchicalRegimeVQVAE is currently used for encoding.
       * Search: Look for vqvae.hierarchical_encode calls.
       * Critical Questions:
           * How is the mp_features dictionary constructed and passed to the VQ-VAE?
           * What is the source of the raw time series data that HierarchicalStreamingMP processes?

   4. Read `run_core.py`:
       * Purpose: Understand how HierarchicalRegimeVQVAE is instantiated and configured.
       * Critical Questions:
           * What are the scales, input_dim, latent_dim, and codebook_sizes parameters used for VQ-VAE instantiation?
           * Where would a training script logically fit within the project structure?

   5. Review `gemini_docs/MODEL_STRUCTURE.md` and `gemini_docs/ARCHITECTURAL_CONCEPTS.md`:
       * Purpose: Reconfirm the theoretical underpinnings of the HierarchicalRegimeVQVAE, especially regarding loss functions, codebook management, and hierarchical
         conditioning.
       * Search: Look for keywords like "loss," "optimizer," "training," "codebook update," "reconstruction."
       * Critical Questions:
           * Are there specific loss components (e.g., reconstruction, commitment, codebook regularization) that need to be implemented?
           * What are the expected interactions between hierarchical layers during training (e.g., how does the loss from one layer affect another)?

   6. Search for existing training scripts:
       * Search: glob for train_*.py or similar patterns in the scripts/ directory.
       * Purpose: Identify existing training patterns, data loading mechanisms, and common utility functions that could be reused or adapted.

  Critical Questions to Answer Before Work Begins:

   * Loss Function Formulation: What are the precise mathematical formulations for the reconstruction loss, VQ commitment loss, and any other regularization terms (e.g.,
     codebook loss, diversity loss) for the HierarchicalRegimeVQVAE? How are these combined into a total loss?
   * Codebook Update Mechanism: How will the codebooks within each ContinualVQVAELayer be updated during training (e.g., exponential moving average, k-means)?
   * Data Generation/Loading: What is the strategy for generating or loading suitable time series data for training? Will it be synthetic, historical market data, or a
     combination? How will this data be preprocessed into the mp_features format expected by the VQ-VAE?
   * Training Loop Structure: What is the high-level structure of the training loop (e.g., epochs, batches, forward pass, loss calculation, backward pass, optimizer step,
     codebook update, logging)?
   * Hyperparameters: What are the initial hyperparameters for the optimizer (learning rate, betas), loss weights, and VQ-VAE specific parameters (e.g., commitment cost)?

  3. Proposed Strategic Approach

  The development of the training loop will follow a structured, iterative approach, focusing on getting a minimal viable training process functional before adding
  complexity.

  Phase 1: Data Preparation and Basic VQ-VAE Forward Pass

  Objective: Establish a reliable way to feed data to the VQ-VAE and verify its forward pass.

   1. Data Source Identification: Determine the primary source of training data (e.g., data/market_data.csv, synthetic data).
   2. Data Loading and Preprocessing:
       * Implement a data loader (e.g., a PyTorch Dataset and DataLoader) to load raw time series data.
       * Integrate HierarchicalStreamingMP (or a batch-equivalent for offline training) to generate mp_features from the raw time series data. This will involve simulating
         the streaming process or adapting HierarchicalStreamingMP for batch processing.
   3. VQ-VAE Input Formatting: Ensure the mp_features are correctly formatted (e.g., Dict[int, torch.Tensor]) as expected by HierarchicalRegimeVQVAE.forward.
   4. Initial Forward Pass Test: Instantiate HierarchicalRegimeVQVAE and perform a forward pass with dummy or preprocessed data to ensure the model runs without errors and
      produces expected outputs (reconstructions, quantized latents, VQ losses).

  Phase 2: Loss Function Implementation and Single-Step Optimization

  Objective: Implement the core loss functions and verify a single optimization step.

   1. Loss Function Definition:
       * Implement the reconstruction loss (e.g., MSE between input mp_features and reconstructed mp_features).
       * Implement the VQ commitment loss and codebook loss as described in VQ-VAE literature and MODEL_STRUCTURE.md.
       * Combine these into a total loss with configurable weights.
   2. Optimizer Setup: Initialize an optimizer (e.g., torch.optim.Adam) for the VQ-VAE parameters.
   3. Single Batch Training Step:
       * Load a single batch of preprocessed data.
       * Perform a forward pass.
       * Calculate the total loss.
       * Perform loss.backward() and optimizer.step().
       * Implement the codebook update mechanism (e.g., EMA or k-means) for each ContinualVQVAELayer.
       * Verify that model parameters and codebooks are updating.

  Phase 3: Full Training Loop and Basic Logging

  Objective: Construct the complete training loop and add basic monitoring.

   1. Epoch and Batch Iteration: Implement the outer loops for iterating through epochs and batches of data.
   2. Training and Validation Phases: Structure the loop to include distinct training and (optional) validation phases.
   3. Logging: Add basic logging for loss values, learning rate, and potentially codebook usage statistics per epoch/batch.
   4. Model Saving/Loading: Implement functionality to save and load the trained HierarchicalRegimeVQVAE model state (including optimizer state and codebooks).
   5. Hyperparameter Management: Centralize hyperparameters (learning rate, loss weights, VQ-VAE specific parameters) for easy configuration.

  4. Verification Strategy

  Success will be measured by the model's ability to train, reduce loss, and produce meaningful outputs, along with robust testing.

   1. Unit Tests for Loss Functions:
       * Create unit tests for each individual loss component (reconstruction, commitment, codebook loss) to ensure they are calculated correctly.
   2. Unit Tests for Codebook Update:
       * Create unit tests to verify that the codebook update mechanism (e.g., EMA) correctly modifies the codebook embeddings.
   3. Integration Test (Mini-Training Run):
       * Create an integration test that performs a small number of training steps (e.g., 10-20 batches) with dummy data.
       * Assert that the total loss decreases over these steps.
       * Assert that model parameters and codebook embeddings are updated.
       * Assert that the VQ-VAE produces valid reconstructions and quantized latents.
   4. Logging and Visualization:
       * Monitor training progress through logging of loss values.
       * (Optional, but highly recommended) Integrate a tool like TensorBoard to visualize loss curves, codebook usage, and potentially reconstructions.
   5. Full Regression Test Suite:
       * After implementing the training loop, run pytest /home/chris/starchild/tests to ensure no regressions were introduced in other parts of the system.

  5. Anticipated Challenges & Considerations

   * Data Generation/Preprocessing Complexity: Generating realistic mp_features for training can be complex, especially ensuring they represent diverse regimes. Adapting
     HierarchicalStreamingMP for efficient batch processing might be non-trivial.
   * VQ-VAE Loss Balancing: Tuning the weights for reconstruction loss, commitment loss, and codebook loss can be challenging. Incorrect balancing can lead to codebook
     collapse or poor reconstruction.
   * Codebook Initialization and Update: The initialization strategy for codebook embeddings and the choice of update mechanism (e.g., EMA vs. k-means) can significantly
     impact training stability and performance.
   * Hierarchical Loss Propagation: Designing how losses are combined and backpropagated through the hierarchical layers of the VQ-VAE, especially with conditioning,
     requires careful implementation.
   * Computational Resources: Training VQ-VAEs, especially hierarchical ones, can be computationally intensive and require GPU resources.
   * Debugging VQ-VAE: Debugging VQ-VAEs can be notoriously difficult due to the discrete nature of the vector quantization step.
   * Hyperparameter Tuning: The training loop will introduce many new hyperparameters that will require extensive tuning for optimal performance.
   * Integration with Continual Learning (EWC): While EWC is a subsequent step, the training loop should be designed with extensibility in mind to easily integrate EWC's
     loss terms and Fisher matrix computations later.
   * Model Persistence: Ensuring that the entire state of the VQ-VAE (including codebooks) can be correctly saved and loaded is crucial for resuming training or deploying
     the model.
